{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------Install Dependencies----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn\n",
    "# !pip install -U spacy\n",
    "# !python -m spacy download en\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------Train Data----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_neg_train= \"Desktop/ML/train/neg\"\n",
    "path_pos_train= \"Desktop/ML/train/pos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_neg_train_files= os.listdir(path_neg_train) \n",
    "all_pos_train_files= os.listdir(path_pos_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new list for negative and positive train data\n",
    "new_neg_train_list=[]\n",
    "new_pos_train_list=[]\n",
    "\n",
    "# To read and open the all text files \n",
    "for fle in all_neg_train_files:\n",
    "    with open(os.path.join(path_neg_train, fle)) as f1:\n",
    "        text1=f1.read()\n",
    "        new_neg_train_list.append(text1)\n",
    "     \n",
    "    \n",
    "    \n",
    "for ijk in all_pos_train_files:\n",
    "       with open(os.path.join(path_pos_train, ijk)) as f2:\n",
    "            text2=f2.read()\n",
    "            new_pos_train_list.append(text2) \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting into dataframe\n",
    "df_neg_train = pd.DataFrame(new_neg_train_list)\n",
    "df_pos_train = pd.DataFrame(new_pos_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert new column to add sentiments manually\n",
    "df_neg_train.insert(1,\"new_col\",\"0\")  # Zero/\"0\"is used for negative sentiments\n",
    "df_pos_train.insert(1,\"new_col\",\"1\")  # One/\"1\" is used for positive sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naming Column Names \n",
    "columns_name=['Review','Sentiment']\n",
    "df_neg_train.columns=columns_name \n",
    "df_pos_train.columns=columns_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12500, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Understanding the shape of dataframe of negative and positive\n",
    "df_neg_train.shape\n",
    "df_pos_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining Negative and Positive train data into one file \n",
    "final_train_data= df_neg_train.append(df_pos_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Final Train Data----\n",
      "                                                  Review Sentiment\n",
      "0      Working with one of the best Shakespeare sourc...         0\n",
      "1      Well...tremors I, the original started off in ...         0\n",
      "2      Ouch! This one was a bit painful to sit throug...         0\n",
      "3      I've seen some crappy movies in my life, but t...         0\n",
      "4      \"Carriers\" follows the exploits of two guys an...         0\n",
      "12495  About a year ago I finally gave up on American...         1\n",
      "12496  When I saw the elaborate DVD box for this and ...         1\n",
      "12497  Last November, I had a chance to see this film...         1\n",
      "12498  Great movie -I loved it. Great editing and use...         1\n",
      "12499  Enchanted April is a tone poem, an impressioni...         1\n"
     ]
    }
   ],
   "source": [
    "a=final_train_data.head().append(final_train_data.tail())\n",
    "print(\"----Final Train Data----\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ------Test Data---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_neg_test= \"Desktop/ML/test/neg\"\n",
    "path_pos_test= \"Desktop/ML/test/pos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_neg_test_files= os.listdir(path_neg_test) \n",
    "#print(all_neg_test_files) \n",
    "all_pos_test_files= os.listdir(path_pos_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new list for negative and positive train data\n",
    "new_neg_test_list=[]\n",
    "new_pos_test_list=[]\n",
    "\n",
    "# To read and open the all text files\n",
    "for fle in all_neg_test_files:\n",
    "    with open(os.path.join(path_neg_test, fle)) as f1:\n",
    "        text1=f1.read()\n",
    "        new_neg_test_list.append(text1)  \n",
    "        \n",
    "        \n",
    "for ijk in all_pos_test_files:\n",
    "    with open(os.path.join(path_pos_test, ijk)) as f2:\n",
    "        text2=f2.read()\n",
    "        new_pos_test_list.append(text2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting into dataframe\n",
    "df_neg_test = pd.DataFrame(new_neg_test_list)\n",
    "df_pos_test = pd.DataFrame(new_pos_test_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert new column to add sentiments manually\n",
    "df_neg_test.insert(1,\"new_col\",\"0\") \n",
    "df_pos_test.insert(1,\"new_col\",\"1\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naming Column Names\n",
    "columns_name=['Review','Sentiment']\n",
    "df_neg_test.columns=columns_name \n",
    "df_pos_test.columns=columns_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12500, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting into dataframe\n",
    "df_neg_test.shape\n",
    "df_pos_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining Negative and Positive test data into one file \n",
    "final_test_data= df_neg_test.append(df_pos_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Final Test Data-----\n",
      "                                                  Review Sentiment\n",
      "0      Alan Rickman & Emma Thompson give good perform...         0\n",
      "1      I have seen this movie and I did not care for ...         0\n",
      "2      In Los Angeles, the alcoholic and lazy Hank Ch...         0\n",
      "3      This film is bundled along with \"Gli fumavano ...         0\n",
      "4      I only comment on really very good films and o...         0\n",
      "12495  This movie is certainly well-constructed, begi...         1\n",
      "12496  Nice to see a comedy for grown ups. Masterfull...         1\n",
      "12497  Jean Renoir's homage to the Paris of the late ...         1\n",
      "12498  What are the movies? I mean.. what are movies ...         1\n",
      "12499  I saw this movie on TV and loved it! I am a re...         1\n"
     ]
    }
   ],
   "source": [
    "b=final_test_data.head().append(final_test_data.tail())\n",
    "print(\"----Final Test Data-----\")\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Review       0\n",
       "Sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test_data.isnull().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    12500\n",
       "0    12500\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test_data['Sentiment'].value_counts() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------Combined Test and Train Data--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = final_train_data.append(final_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Working with one of the best Shakespeare sourc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Well...tremors I, the original started off in ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ouch! This one was a bit painful to sit throug...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've seen some crappy movies in my life, but t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Carriers\" follows the exploits of two guys an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12495</th>\n",
       "      <td>This movie is certainly well-constructed, begi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12496</th>\n",
       "      <td>Nice to see a comedy for grown ups. Masterfull...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12497</th>\n",
       "      <td>Jean Renoir's homage to the Paris of the late ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12498</th>\n",
       "      <td>What are the movies? I mean.. what are movies ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12499</th>\n",
       "      <td>I saw this movie on TV and loved it! I am a re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Review Sentiment\n",
       "0      Working with one of the best Shakespeare sourc...         0\n",
       "1      Well...tremors I, the original started off in ...         0\n",
       "2      Ouch! This one was a bit painful to sit throug...         0\n",
       "3      I've seen some crappy movies in my life, but t...         0\n",
       "4      \"Carriers\" follows the exploits of two guys an...         0\n",
       "12495  This movie is certainly well-constructed, begi...         1\n",
       "12496  Nice to see a comedy for grown ups. Masterfull...         1\n",
       "12497  Jean Renoir's homage to the Paris of the late ...         1\n",
       "12498  What are the movies? I mean.. what are movies ...         1\n",
       "12499  I saw this movie on TV and loved it! I am a re...         1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.head().append(final_data.tail()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---- Introduction to Spacy ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "from spacy import displacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(str(final_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "Review\n",
      "Sentiment\n",
      "\n",
      "\n",
      "0\n",
      "     \n",
      "Working\n",
      "with\n",
      "one\n",
      "of\n",
      "the\n",
      "best\n",
      "Shakespeare\n",
      "sourc\n",
      "...\n",
      "        \n",
      "0\n",
      "\n",
      "\n",
      "1\n",
      "     \n",
      "Well\n",
      "...\n",
      "tremors\n",
      "I\n",
      ",\n",
      "the\n",
      "original\n",
      "started\n",
      "off\n",
      "in\n",
      "...\n",
      "        \n",
      "0\n",
      "\n",
      "\n",
      "2\n",
      "     \n",
      "Ouch\n",
      "!\n",
      "This\n",
      "one\n",
      "was\n",
      "a\n",
      "bit\n",
      "painful\n",
      "to\n",
      "sit\n",
      "throug\n",
      "...\n",
      "        \n",
      "0\n",
      "\n",
      "\n",
      "3\n",
      "     \n",
      "I\n",
      "'ve\n",
      "seen\n",
      "some\n",
      "crappy\n",
      "movies\n",
      "in\n",
      "my\n",
      "life\n",
      ",\n",
      "but\n",
      "t\n",
      "...\n",
      "        \n",
      "0\n",
      "\n",
      "\n",
      "4\n",
      "     \n",
      "\"\n",
      "Carriers\n",
      "\"\n",
      "follows\n",
      "the\n",
      "exploits\n",
      "of\n",
      "two\n",
      "guys\n",
      "an\n",
      "...\n",
      "        \n",
      "0\n",
      "\n",
      "\n",
      "...\n",
      "                                                 \n",
      "...\n",
      "      \n",
      "...\n",
      "\n",
      "\n",
      "12495\n",
      " \n",
      "This\n",
      "movie\n",
      "is\n",
      "certainly\n",
      "well\n",
      "-\n",
      "constructed\n",
      ",\n",
      "begi\n",
      "...\n",
      "        \n",
      "1\n",
      "\n",
      "\n",
      "12496\n",
      " \n",
      "Nice\n",
      "to\n",
      "see\n",
      "a\n",
      "comedy\n",
      "for\n",
      "grown\n",
      "ups\n",
      ".\n",
      "Masterfull\n",
      "...\n",
      "        \n",
      "1\n",
      "\n",
      "\n",
      "12497\n",
      " \n",
      "Jean\n",
      "Renoir\n",
      "'s\n",
      "homage\n",
      "to\n",
      "the\n",
      "Paris\n",
      "of\n",
      "the\n",
      "late\n",
      "...\n",
      "        \n",
      "1\n",
      "\n",
      "\n",
      "12498\n",
      " \n",
      "What\n",
      "are\n",
      "the\n",
      "movies\n",
      "?\n",
      "I\n",
      "mean\n",
      "..\n",
      "what\n",
      "are\n",
      "movies\n",
      "...\n",
      "        \n",
      "1\n",
      "\n",
      "\n",
      "12499\n",
      " \n",
      "I\n",
      "saw\n",
      "this\n",
      "movie\n",
      "on\n",
      "TV\n",
      "and\n",
      "loved\n",
      "it\n",
      "!\n",
      "I\n",
      "am\n",
      "a\n",
      "re\n",
      "...\n",
      "        \n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "[\n",
      "50000\n",
      "rows\n",
      "x\n",
      "2\n",
      "columns\n",
      "]\n",
      "                                                  Review Sentiment\n",
      "\n",
      "0      \n",
      "Working with one of the best Shakespeare sourc...         \n",
      "0\n",
      "\n",
      "1      \n",
      "Well...tremors\n",
      "I, the original started off in ...         0\n",
      "\n",
      "2      Ouch!\n",
      "This one was a bit painful to sit throug...         \n",
      "0\n",
      "\n",
      "3      \n",
      "I've seen some crappy movies in my life, but t...         0\n",
      "\n",
      "4      \"Carriers\" follows the exploits of two guys an...         0\n",
      "...                                                  \n",
      "...       ...\n",
      "\n",
      "12495  \n",
      "This movie is certainly well-constructed, begi...         \n",
      "1\n",
      "\n",
      "12496  Nice to see a comedy for grown ups.\n",
      "Masterfull...         \n",
      "1\n",
      "\n",
      "12497  \n",
      "Jean Renoir's homage to the Paris of the late ...         \n",
      "1\n",
      "\n",
      "12498  \n",
      "What are the movies?\n",
      "I mean.. what are movies ...         \n",
      "1\n",
      "\n",
      "12499  \n",
      "I saw this movie on TV and loved it!\n",
      "I am a re...         \n",
      "1\n",
      "\n",
      "\n",
      "[50000 rows x 2 columns]\n",
      "                                                  \n",
      "Review\n",
      "Sentiment\n",
      "\n",
      "\n",
      "0\n",
      "     \n",
      "Working\n",
      "best\n",
      "Shakespeare\n",
      "sourc\n",
      "...\n",
      "        \n",
      "0\n",
      "\n",
      "\n",
      "1\n",
      "     \n",
      "...\n",
      "tremors\n",
      ",\n",
      "original\n",
      "started\n",
      "...\n",
      "        \n",
      "0\n",
      "\n",
      "\n",
      "2\n",
      "     \n",
      "Ouch\n",
      "!\n",
      "bit\n",
      "painful\n",
      "sit\n",
      "throug\n",
      "...\n",
      "        \n",
      "0\n",
      "\n",
      "\n",
      "3\n",
      "     \n",
      "seen\n",
      "crappy\n",
      "movies\n",
      "life\n",
      ",\n",
      "t\n",
      "...\n",
      "        \n",
      "0\n",
      "\n",
      "\n",
      "4\n",
      "     \n",
      "\"\n",
      "Carriers\n",
      "\"\n",
      "follows\n",
      "exploits\n",
      "guys\n",
      "...\n",
      "        \n",
      "0\n",
      "\n",
      "\n",
      "...\n",
      "                                                 \n",
      "...\n",
      "      \n",
      "...\n",
      "\n",
      "\n",
      "12495\n",
      " \n",
      "movie\n",
      "certainly\n",
      "-\n",
      "constructed\n",
      ",\n",
      "begi\n",
      "...\n",
      "        \n",
      "1\n",
      "\n",
      "\n",
      "12496\n",
      " \n",
      "Nice\n",
      "comedy\n",
      "grown\n",
      "ups\n",
      ".\n",
      "Masterfull\n",
      "...\n",
      "        \n",
      "1\n",
      "\n",
      "\n",
      "12497\n",
      " \n",
      "Jean\n",
      "Renoir\n",
      "homage\n",
      "Paris\n",
      "late\n",
      "...\n",
      "        \n",
      "1\n",
      "\n",
      "\n",
      "12498\n",
      " \n",
      "movies\n",
      "?\n",
      "mean\n",
      "..\n",
      "movies\n",
      "...\n",
      "        \n",
      "1\n",
      "\n",
      "\n",
      "12499\n",
      " \n",
      "saw\n",
      "movie\n",
      "TV\n",
      "loved\n",
      "!\n",
      "...\n",
      "        \n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "[\n",
      "50000\n",
      "rows\n",
      "x\n",
      "2\n",
      "columns\n",
      "]\n",
      "                                                                                                     \n",
      "Review Review\n",
      "Sentiment Sentiment\n",
      "\n",
      " \n",
      "\n",
      "0 0\n",
      "           \n",
      "Working work\n",
      "with with\n",
      "one one\n",
      "of of\n",
      "the the\n",
      "best good\n",
      "Shakespeare Shakespeare\n",
      "sourc sourc\n",
      "... ...\n",
      "                 \n",
      "0 0\n",
      "\n",
      " \n",
      "\n",
      "1 1\n",
      "           \n",
      "Well well\n",
      "... ...\n",
      "tremors tremor\n",
      "I -PRON-\n",
      ", ,\n",
      "the the\n",
      "original original\n",
      "started start\n",
      "off off\n",
      "in in\n",
      "... ...\n",
      "                 \n",
      "0 0\n",
      "\n",
      " \n",
      "\n",
      "2 2\n",
      "           \n",
      "Ouch ouch\n",
      "! !\n",
      "This this\n",
      "one one\n",
      "was be\n",
      "a a\n",
      "bit bit\n",
      "painful painful\n",
      "to to\n",
      "sit sit\n",
      "throug throug\n",
      "... ...\n",
      "                 \n",
      "0 0\n",
      "\n",
      " \n",
      "\n",
      "3 3\n",
      "           \n",
      "I -PRON-\n",
      "'ve have\n",
      "seen see\n",
      "some some\n",
      "crappy crappy\n",
      "movies movie\n",
      "in in\n",
      "my -PRON-\n",
      "life life\n",
      ", ,\n",
      "but but\n",
      "t t\n",
      "... ...\n",
      "                 \n",
      "0 0\n",
      "\n",
      " \n",
      "\n",
      "4 4\n",
      "           \n",
      "\" \"\n",
      "Carriers carrier\n",
      "\" \"\n",
      "follows follow\n",
      "the the\n",
      "exploits exploit\n",
      "of of\n",
      "two two\n",
      "guys guy\n",
      "an an\n",
      "... ...\n",
      "                 \n",
      "0 0\n",
      "\n",
      " \n",
      "\n",
      "... ...\n",
      "                                                                                                   \n",
      "... ...\n",
      "             \n",
      "... ...\n",
      "\n",
      " \n",
      "\n",
      "12495 12495\n",
      "   \n",
      "This this\n",
      "movie movie\n",
      "is be\n",
      "certainly certainly\n",
      "well well\n",
      "- -\n",
      "constructed construct\n",
      ", ,\n",
      "begi begi\n",
      "... ...\n",
      "                 \n",
      "1 1\n",
      "\n",
      " \n",
      "\n",
      "12496 12496\n",
      "   \n",
      "Nice Nice\n",
      "to to\n",
      "see see\n",
      "a a\n",
      "comedy comedy\n",
      "for for\n",
      "grown grow\n",
      "ups up\n",
      ". .\n",
      "Masterfull Masterfull\n",
      "... ...\n",
      "                 \n",
      "1 1\n",
      "\n",
      " \n",
      "\n",
      "12497 12497\n",
      "   \n",
      "Jean Jean\n",
      "Renoir Renoir\n",
      "'s 's\n",
      "homage homage\n",
      "to to\n",
      "the the\n",
      "Paris Paris\n",
      "of of\n",
      "the the\n",
      "late late\n",
      "... ...\n",
      "                 \n",
      "1 1\n",
      "\n",
      " \n",
      "\n",
      "12498 12498\n",
      "   \n",
      "What what\n",
      "are be\n",
      "the the\n",
      "movies movie\n",
      "? ?\n",
      "I -PRON-\n",
      "mean mean\n",
      ".. ..\n",
      "what what\n",
      "are be\n",
      "movies movie\n",
      "... ...\n",
      "                 \n",
      "1 1\n",
      "\n",
      " \n",
      "\n",
      "12499 12499\n",
      "   \n",
      "I -PRON-\n",
      "saw see\n",
      "this this\n",
      "movie movie\n",
      "on on\n",
      "TV tv\n",
      "and and\n",
      "loved love\n",
      "it -PRON-\n",
      "! !\n",
      "I -PRON-\n",
      "am be\n",
      "a a\n",
      "re re\n",
      "... ...\n",
      "                 \n",
      "1 1\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "[ [\n",
      "50000 50000\n",
      "rows row\n",
      "x x\n",
      "2 2\n",
      "columns column\n",
      "] ]\n",
      "                                                   SPACE\n",
      "Review PROPN\n",
      "Sentiment PROPN\n",
      "\n",
      " SPACE\n",
      "0 NUM\n",
      "      SPACE\n",
      "Working VERB\n",
      "with ADP\n",
      "one NUM\n",
      "of ADP\n",
      "the DET\n",
      "best ADJ\n",
      "Shakespeare PROPN\n",
      "sourc NOUN\n",
      "... PUNCT\n",
      "         SPACE\n",
      "0 NUM\n",
      "\n",
      " SPACE\n",
      "1 NUM\n",
      "      SPACE\n",
      "Well INTJ\n",
      "... PUNCT\n",
      "tremors NOUN\n",
      "I PRON\n",
      ", PUNCT\n",
      "the DET\n",
      "original NOUN\n",
      "started VERB\n",
      "off ADP\n",
      "in ADV\n",
      "... PUNCT\n",
      "         SPACE\n",
      "0 NUM\n",
      "\n",
      " SPACE\n",
      "2 NUM\n",
      "      SPACE\n",
      "Ouch ADJ\n",
      "! PUNCT\n",
      "This DET\n",
      "one NOUN\n",
      "was AUX\n",
      "a DET\n",
      "bit NOUN\n",
      "painful ADJ\n",
      "to PART\n",
      "sit VERB\n",
      "throug PROPN\n",
      "... PUNCT\n",
      "         SPACE\n",
      "0 NUM\n",
      "\n",
      " SPACE\n",
      "3 NUM\n",
      "      SPACE\n",
      "I PRON\n",
      "'ve AUX\n",
      "seen VERB\n",
      "some DET\n",
      "crappy ADJ\n",
      "movies NOUN\n",
      "in ADP\n",
      "my DET\n",
      "life NOUN\n",
      ", PUNCT\n",
      "but CCONJ\n",
      "t NOUN\n",
      "... PUNCT\n",
      "         SPACE\n",
      "0 PUNCT\n",
      "\n",
      " SPACE\n",
      "4 NUM\n",
      "      SPACE\n",
      "\" PUNCT\n",
      "Carriers NOUN\n",
      "\" PUNCT\n",
      "follows VERB\n",
      "the DET\n",
      "exploits NOUN\n",
      "of ADP\n",
      "two NUM\n",
      "guys NOUN\n",
      "an DET\n",
      "... PUNCT\n",
      "         SPACE\n",
      "0 PUNCT\n",
      "\n",
      " SPACE\n",
      "... PUNCT\n",
      "                                                  SPACE\n",
      "... PUNCT\n",
      "       SPACE\n",
      "... PUNCT\n",
      "\n",
      " SPACE\n",
      "12495 NUM\n",
      "  SPACE\n",
      "This DET\n",
      "movie NOUN\n",
      "is AUX\n",
      "certainly ADV\n",
      "well ADV\n",
      "- PUNCT\n",
      "constructed VERB\n",
      ", PUNCT\n",
      "begi PROPN\n",
      "... PUNCT\n",
      "         SPACE\n",
      "1 NUM\n",
      "\n",
      " SPACE\n",
      "12496 NUM\n",
      "  SPACE\n",
      "Nice PROPN\n",
      "to PART\n",
      "see VERB\n",
      "a DET\n",
      "comedy NOUN\n",
      "for ADP\n",
      "grown VERB\n",
      "ups NOUN\n",
      ". PUNCT\n",
      "Masterfull PROPN\n",
      "... PUNCT\n",
      "         SPACE\n",
      "1 NUM\n",
      "\n",
      " SPACE\n",
      "12497 NUM\n",
      "  SPACE\n",
      "Jean PROPN\n",
      "Renoir PROPN\n",
      "'s PART\n",
      "homage NOUN\n",
      "to ADP\n",
      "the DET\n",
      "Paris PROPN\n",
      "of ADP\n",
      "the DET\n",
      "late ADJ\n",
      "... PUNCT\n",
      "         SPACE\n",
      "1 NUM\n",
      "\n",
      " SPACE\n",
      "12498 NUM\n",
      "  SPACE\n",
      "What PRON\n",
      "are AUX\n",
      "the DET\n",
      "movies NOUN\n",
      "? PUNCT\n",
      "I PRON\n",
      "mean VERB\n",
      ".. PUNCT\n",
      "what PRON\n",
      "are AUX\n",
      "movies NOUN\n",
      "... PUNCT\n",
      "         SPACE\n",
      "1 NUM\n",
      "\n",
      " SPACE\n",
      "12499 NUM\n",
      "  SPACE\n",
      "I PRON\n",
      "saw VERB\n",
      "this DET\n",
      "movie NOUN\n",
      "on ADP\n",
      "TV NOUN\n",
      "and CCONJ\n",
      "loved VERB\n",
      "it PRON\n",
      "! PUNCT\n",
      "I PRON\n",
      "am AUX\n",
      "a DET\n",
      "re NOUN\n",
      "... PUNCT\n",
      "         SPACE\n",
      "1 NUM\n",
      "\n",
      "\n",
      " SPACE\n",
      "[ PUNCT\n",
      "50000 NUM\n",
      "rows NOUN\n",
      "x SYM\n",
      "2 NUM\n",
      "columns NOUN\n",
      "] PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)\n",
    "    \n",
    "    \n",
    "sent = nlp.create_pipe('sentencizer')\n",
    "nlp.add_pipe(sent, before='parser')\n",
    "\n",
    "\n",
    "\n",
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "    \n",
    "    \n",
    "stopwords = list(STOP_WORDS)\n",
    "# len(stopwords)\n",
    "\n",
    "#To remove the stop words \n",
    "for token in doc:\n",
    "    if token.is_stop == False:\n",
    "        print(token)\n",
    "        \n",
    "for lem in doc:\n",
    "    print(lem.text, lem.lemma_)\n",
    "    \n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)\n",
    "    \n",
    "punct = string.punctuation\n",
    "# print(punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----Vectorization----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_data_cleaning(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if token.lemma_ != \"-PRON-\":\n",
    "            temp = token.lemma_.lower().strip()\n",
    "        else:\n",
    "            temp = token.lower_\n",
    "        tokens.append(temp)\n",
    "    \n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in stopwords and token not in punct:\n",
    "            cleaned_tokens.append(token)\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer = text_data_cleaning)\n",
    "classifier = LinearSVC()\n",
    "\n",
    "#h = 250\n",
    "#t = 250\n",
    "X = final_data['Review']            #for whole data\n",
    "y = final_data['Sentiment']          #for whole data \n",
    "#X = final_data['Review'].head(h).append(final_data['Review'].tail(t), ignore_index=True)\n",
    "#y = final_data['Sentiment'].head(h).append(final_data['Sentiment'].tail(t), ignore_index=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40000,), (10000,), (40000,), (10000,))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape , y_train.shape , y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline\n",
    "clf = Pipeline([('tfidf', tfidf), ('clf', classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction\n",
    "y_pred = clf.predict(X_test)\n",
    "#print(ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89      4965\n",
      "           1       0.89      0.90      0.89      5035\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4391,  574],\n",
       "       [ 511, 4524]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8915"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To calculate the accuracy\n",
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----Validation-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1'], dtype=object)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(['Wow, this is amzing lesson'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0'], dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(['Alas, this is worst lesson'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0'], dtype=object)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(['Wow, this sucks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1'], dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(['Loved it. amazing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
